\documentclass[11pt,a4paper]{article}
\usepackage{geometry,cmbright,graphicx,amsmath,amsfonts,amssymb,calc}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\newcommand{\tp}{\top}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\Lag}{\mathcal{L}}

\title{Distributed ADMM : Thoughts for Implementation via YARP}
\date{29/3/2015}
\author{C.N. Jones}

\begin{document}
\maketitle

We want to develop a distributed operator splitting code to solve problems of the form
\begin{align}\label{eqn:central}
  \min\ & \sum f_i(x_i) + g\left(\sum F_i x_i + h\right)
\end{align}
where the $f_i$'s are functions of the form
\begin{align*}
f_i(x_i) = \hat f_i(x_i) + \sum w_{ij} q_{ij}(T_{ij}x_i + t_{ij}) + 1_{A_ix_i=b_i}(x_i)
\end{align*}

% \begin{align}\label{eqn:central}
%   \min\ & f(x) + g(z) + \sum w_j q_j(y_j)\\
%   \text{s.t.}\ & 
%   \begin{aligned}[t]
%   Ax & = b\\
%   y_j &= T_jx + t_j\\
%   F x + h &= z
%   \end{aligned}
% \end{align}
% where $f$, $q_j$ are separable functions, $A$ and $T_j$ are block diagonal. The matrix $G$ is $\begin{bmatrix}I&I&\cdots&I\end{bmatrix}^T$.

% The expanded form of the problem:
% \begin{align*}
%   \min\ & \sum f_i(x_i) + g(z) + \sum w_{ij} q_{ij}(y_{ij})\\
%   \text{s.t.}\ & 
%   \begin{aligned}[t]
%   A_ix_i & = b_i\\
%   y_{ij} &= T_{ij}x_i + t_{ij}\\
%   \sum F_i x_i + h &= z
%   \end{aligned}
% \end{align*}

\fbox{\parbox{\textwidth-2\fboxsep-2\fboxrule}{Question 1: Is this the best form of the problem? Is it sufficiently general?}}

\section*{Example}
An example problem we can use to develop / test the code is the distributed tracking problem for multiple buildings. Each local problem is an MPC problem for the building, and the goal is to enforce that the total input power sums to a given amount.

In this scenario, the $A_i$ matrices come from the dynamic constraints of building $i$, and the $q_i$ functions would be the indicator function for the positive orthant.

The $F_i$ matrices would simply be the sum of the inputs at each stage along the prediction horizon, $h = 0$, making $\sum F_i x_i$ the total power consumed by the aggregation along the horizon. The function $g$ would then be an appropriate distance measure from the desired power trajectory.

\section*{Distributed Formulations}
The following two sections outline two obvious ADMM formulations for this problem, and discusses how these could be implemented using the current SPLIT toolbox.

% For reference, the augmented Lagrangian is:
% \begin{align*}
% \Lag = f(x) + g(z) + \sum w_jq_j(y_j) + 1_A(x) + \frac{\rho}{2}\sum_j \|T_jx + t_j - y_j + \lambda_j\|^2 + \frac{\rho}{2}\|Fx + h - Gz + \nu\|^2
% \end{align*}

\subsection*{Expensive Communication, Cheap Computation}
Here we would want to spend more time computing per communication step. 

% We define the function $\hat f$ as
% \begin{align*}
% \hat f(x) = f(x) + \sum w_j q_j(Tx + t) + 1_{Ax=b}(x)
% \end{align*}

Problem~\eqref{eqn:central} can be written as
\begin{align*}
  \min\ & \sum f_i(x_i) + g(z)\\
  \text{s.t.}\ & 
  \begin{aligned}[t]
  F_i x_i &= u_i\\
  \sum u_i + h &= z\\
  \end{aligned}
\end{align*}

This gives us the augmented Lagrangian:
\begin{align*}
  \min\ \sum f_i(x_i) + g(z) + \sum \frac{\rho}{2}\|F_ix_i - u_i + \lambda_i\|^2 + \frac{\rho}{2}\|\sum u_i - z + \nu\|^2
\end{align*}

If we apply ADMM to this problem, we get:
\begin{subequations}
\begin{align}
\label{step:x}x_i^{k+1} &= \argmin_{x_i}\ f_i(x_i) + \frac{\rho}{2}\|F_ix_i - u_i^k + \lambda_i^k\|^2\\
\label{step:z}z^{k+1} &= \argmin_z\ g(z) + \frac{\rho}{2}\|\sum u_i^{k+1} - z + \nu^k\|^2\\
\label{step:u}u^{k+1} &= \argmin_{u}\ \sum \|F_ix_i^{k+1} - u_i + \lambda_i^k\|^2 + \|\sum u_i - z^{k+1} + \nu^k\|^2\\
\label{step:lam}\lambda_i^{k+1} &= \lambda_i^k + F_ix_i^{k+1} - u_i^{k+1}\\
\label{step:nu}\nu^{k+1} &= \sum u_i^{k+1} - z^{k+1}
\end{align}
\end{subequations}

Analysing the steps of the algorithm we see:
\begin{description}
  \item[\ref{step:x}] This problem requires the distributed solution of a set of problems parametric in $u_i - \lambda_i$ that are exactly of the form currently generated by SPLIT.
  \item[\ref{step:z}] This step would be solved at a central node (or consensus), and would likely just be a least-squares problem if we're doing power trajectory tracking.
  \item[\ref{step:u}] This step looks complex, but can actually be solved via consensus (or just on a central node). The solution is:
  \begin{align*}
  u &= (I-11^\tp)^{-1}\bar u = \frac{1}{n-1}11^\tp (\bar u + z^{k+1} - \nu^k) + (\bar u + z^{k+1} - \nu^k)
  \end{align*}
  where $\bar u_i = -F_ix_i^{k+1} - \lambda_i^k$
  \item[\ref{step:lam}] Distributes
  \item[\ref{step:nu}] Central node
\end{description}

We see that if the problem is decomposed in this way, we need to solve a complete optimization problem at each node at each iteration. However, the problem itself is exactly a parametric problem of the form that SPLIT currently generates.

Reasons for solving the problem in this way:
\begin{itemize}
  \item The local problems are exactly standard MPC problems. One could imagine that the devices to be controlled may already have the code in place to solve these
  \item The local problems don't need to expose anything about their internal structures. They only ever communicate their planned power trajectories $F_ix_i$ and their cost sensitivities to changing this $\lambda_i$. In practice, this makes for a very simple and standard interface for a wide range of different devices.
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{dsplit}
  \caption{Computational flow for distributed optimization}
  \label{fig:dsplit}
\end{figure}

\subsection*{Cheap Communication, Expensive Computation}
The second method of splitting the problem reduces the computational complexity at each node, but means that the time between communication steps is shorter.

If we expose the details of the inner optimization problems, we get
\begin{align*}
  \min\ & \sum \hat f_i(x_i) + g(z) + \sum w_{ij} q_{ij}(y_{ij})\\
  \text{s.t.}\ & 
  \begin{aligned}[t]
  A_ix_i & = b_i\\
  y_{ij} &= T_{ij}x_i + t_{ij}\\
  F_i x_i &= u_i\\
  \sum u_i + h &= z
  \end{aligned}
\end{align*}

We can see that we can alternate between the pairs of variables $(x_i, z)$ and $(y_{ij}, u_i)$:
\begin{align*}
x_i^{k+1} &= \argmin_{x_i} \hat f_i(x_i) + \frac{\rho}{2}\left( \|A_ix_i - b_i + \lambda_i^k\|^2 + \sum_j\|y_{ij}^k - T_{ij}x_i-t_{ij} + \nu_{ij}^k\|^2 + \|F_ix_i - u_i^k + \gamma_i^k\|^2  \right)\\
z^{k+1} &= \argmin_z g(z) + \frac{\rho}{2}\|\sum u_i^k + h - z + \lambda_z^k\|^2\\
y_{ij}^{k+1} &= \argmin_{y_{ij}} w_{ij}q_{ij}(y_{ij}) + \frac{\rho}{2}\|y_{ij} - T_{ij}x_i^{k+1} - t_{ij} + \nu{ij}^k\|^2\\
u_i^{k+1} &= \argmin_{u_i} \sum \|F_ix_i^{k+1} - u_i + \gamma_i^k\|^2 + \|\sum u_i^k + h - z + \lambda_z^k\|^2\\
& + \text{Dual updates}
\end{align*}

One can see that each step of this algorithm is now only a least-squares solution, which can be pre-factored. 

Upsides:
\begin{itemize}
  \item Computation is much simpler
  \item Data is still hidden - only power consumptions are transmitted to a central node
\end{itemize}

Downsides:
\begin{itemize}
  \item Local nodes (buildings) are only feasible on convergence.
  \item Local nodes require new code. SPLIT does not currently generate a solver than can solve these problems in this form.
  \item Participating in this type of optimization requires more of a structure change to a standard MPC algorithm than the previous method.
\end{itemize}

\fbox{\parbox{\textwidth-2\fboxsep-2\fboxrule}{Question 2: Any reason to believe that one of these formulations might be faster than the other?}}


\end{document}
